{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"text-align: center;\">Representation and Inference of Graphical Models</h1>"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "import pprint\n",
        "import difflib\n",
        "import graphviz\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from itertools import product, combinations\n",
        "from collections import defaultdict, OrderedDict as odict\n",
        "from tabulate import tabulate"
      ],
      "outputs": [],
      "execution_count": 197,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def printFactor(f):\n",
        "    \"\"\"\n",
        "    argument \n",
        "    `f`, a factor to print on screen. If the `ext` field is presented, it will be printed as an additional column\n",
        "    \"\"\"\n",
        "    # Create a empty list that we will fill in with the probability table entries\n",
        "    table = list()\n",
        "    \n",
        "    # Iterate over all keys and probability values in the table\n",
        "    for key, item in f['table'].items():\n",
        "        # Convert the tuple to a list to be able to manipulate it\n",
        "        k = list(key)\n",
        "        # Append the probability value to the list with key values\n",
        "        k.append(item)\n",
        "        \n",
        "        # NEW CODE\n",
        "        # if the 'ext' key is present in the factor, we apppend the ext dictionary converted to a list of items\n",
        "        if 'ext' in f:\n",
        "            k.append(list(f['ext'][key].items()))\n",
        "        \n",
        "        # Append an entire row to the table\n",
        "        table.append(k)\n",
        "    # dom is used as table header. We need it converted to list\n",
        "    dom = list(f['dom'])\n",
        "    # Append a 'Pr' to indicate the probabity column\n",
        "    dom.append('Pr')\n",
        "\n",
        "    print(tabulate(table,headers=dom,tablefmt='orgtbl'))"
      ],
      "outputs": [],
      "execution_count": 198,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def join(f1, f2, outcomeSpace):\n",
        "    \"\"\"\n",
        "    argument\n",
        "    `f1`, first factor to be joined.\n",
        "    `f2`, second factor to be joined.\n",
        "    `outcomeSpace`, dictionary with the domain of each variable\n",
        "\n",
        "    Returns a new factor with a join of f1 and f2. This version tests of the existance of `ext` field and merge them in the resulting factor\n",
        "    \"\"\"\n",
        "    if not f1:\n",
        "        return f2\n",
        "\n",
        "    if not f2:\n",
        "        return f1\n",
        "\n",
        "    # First, we need to determine the domain of the new factor. It will be union of the domain in f1 and f2\n",
        "    # But it is important to eliminate the repetitions\n",
        "    common_vars = list(f1['dom']) + list(set(f2['dom']) - set(f1['dom']))\n",
        "\n",
        "    # We will build a table from scratch, starting with an empty list. Later on, we will transform the list into a odict\n",
        "    table = list()\n",
        "\n",
        "    # Here is where the magic happens. The product iterator will generate all combinations of varible values\n",
        "    # as specified in outcomeSpace. Therefore, it will naturally respect observed values\n",
        "    for entries in product(*[outcomeSpace[node] for node in common_vars]):\n",
        "\n",
        "        # We need to map the entries to the domain of the factors f1 and f2\n",
        "        entryDict = dict(zip(common_vars, entries))\n",
        "        f1_entry = tuple((entryDict[var] for var in f1['dom']))\n",
        "        f2_entry = tuple((entryDict[var] for var in f2['dom']))\n",
        "\n",
        "        # Insert your code here\n",
        "        # Use the fuction prob to calculate the probability in factor f1 for entry f1_entry\n",
        "        p1 = f1['table'][f1_entry]\n",
        "        # Use the fuction prob to calculate the probability in factor f2 for entry f2_entry\n",
        "        p2 = f2['table'][f2_entry]\n",
        "\n",
        "        # Create a new table entry with the multiplication of p1 and p2\n",
        "        table.append((entries, p1 * p2))\n",
        "\n",
        "    return {'dom': tuple(common_vars), 'table': odict(table)}\n"
      ],
      "outputs": [],
      "execution_count": 199,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def marginalize(f, var, outcomeSpace):\n",
        "    \"\"\"\n",
        "    argument\n",
        "    `f`, factor to be marginalized.\n",
        "    `var`, variable to be summed out.\n",
        "    `outcomeSpace`, dictionary with the domain of each variable\n",
        "\n",
        "    Returns a new factor f' with dom(f') = dom(f) - {var}\n",
        "    \"\"\"\n",
        "\n",
        "    # Let's make a copy of f domain and convert it to a list. We need a list to be able to modify its elements\n",
        "    new_dom = list(f['dom'])\n",
        "    # Remove var from the list new_dom by calling the method remove(). 1 line\n",
        "    new_dom.remove(var)\n",
        "    # Create an empty list for table. We will fill in table from scratch. 1 line\n",
        "    table = list()\n",
        "    for entries in product(*[outcomeSpace[node] for node in new_dom]):\n",
        "        s = 0                     # Initialize the summation variable s. 1 line\n",
        "\n",
        "        # We need to iterate over all possible outcomes of the variable var\n",
        "        for val in outcomeSpace[var]:\n",
        "            # To modify the tuple entries, we will need to convert it to a list\n",
        "            entriesList = list(entries)\n",
        "            # We need to insert the value of var in the right position in entriesList\n",
        "            entriesList.insert(f['dom'].index(var), val)\n",
        "\n",
        "            # Calculate the probability of factor f for entriesList. 1 line\n",
        "            p = f['table'][tuple(entriesList)]\n",
        "            # Sum over all values of var by accumulating the sum in s. 1 line\n",
        "            s = s + p\n",
        "\n",
        "        # Create a new table entry with the multiplication of p1 and p2\n",
        "        table.append((entries, s))\n",
        "    return {'dom': tuple(new_dom), 'table': odict(table)}\n"
      ],
      "outputs": [],
      "execution_count": 200,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(f):\n",
        "    \"\"\"\n",
        "    argument\n",
        "    `f`, factor to be normalized.\n",
        "\n",
        "    Returns a new factor f' as a copy of f with entries that sum up to 1\n",
        "    \"\"\"\n",
        "    table = list()\n",
        "    sum = 0\n",
        "    for k, p in f['table'].items():\n",
        "        sum = sum + p\n",
        "    for k, p in f['table'].items():\n",
        "        if sum == 0:\n",
        "            table.append((k, 0))\n",
        "        else:\n",
        "            table.append((k, p/sum))\n",
        "    return {'dom': f['dom'], 'table': odict(table)}\n"
      ],
      "outputs": [],
      "execution_count": 201,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class for Bayes Network\n",
        "### Network Representation:\n",
        "   - child_graph: Adjacency list. Dictionary of Nodes as keys and it's children as its values.\n",
        "   - prob_tables: Probability distribution table\n",
        "   \n",
        "   We follow the hugin file format (.net) which is a text file with two main sections.\n",
        " \n",
        "   ----\n",
        "   The first one specifies the nodes.\n",
        " \n",
        "       node HISTORY\n",
        "       {\n",
        "           states = ( \"TRUE\" \"FALSE\" );\n",
        "       }\n",
        "\n",
        "       node LVFAILURE\n",
        "       {\n",
        "           states = ( \"TRUE\" \"FALSE\" );\n",
        "       }\n",
        "\n",
        "   The second part specifies the probability tables, and has the following format:\n",
        "\n",
        "       potential ( HISTORY | LVFAILURE )\n",
        "       {\n",
        "           data = ((0.9 0.1)(0.01 0.99)) ;\n",
        "       }\n",
        "\n",
        "   ----\n",
        "\n",
        "### Member Functions:\n",
        "- load_network(File)\n",
        " > Loads a hugin (.net) file and parses it to obtain the respective Bayes net.\n",
        "- insert_node(Node, Domain)\n",
        " > Inserts given node into the graph.\n",
        "- remove_node(Node)\n",
        " > Removes the specified node from the graph.\n",
        "- connect_nodes(Node1, Node2)\n",
        " > Connects child node (Node2) to parent node (Node1).\n",
        "- disconnect_nodes()\n",
        " > Disconnects child node (node2) from parent node (Node1).\n",
        "- prob_node(Node, Prob)\n",
        " > Sets probability of the node.\n",
        "- save_network(File)\n",
        " > Saves the Bayes net into a hugin (.net) File.\n",
        "    "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Bayes_Network():\n",
        "    def __init__(self, File):\n",
        "        self.node_pattern = re.compile(\n",
        "            \"\\W+node\\s(\\w*)\\W+states\\W+\\((.*)\\)\\W+\\}\")\n",
        "        self.prob_pattern = re.compile(\n",
        "            \"\\W+potential\\W+\\((.*)\\)\\W+data\\W+\\((.*)\\)\")\n",
        "        self.outcomeSpace = {}\n",
        "        self.child_graph = defaultdict(list)\n",
        "        self.prob_tables = self.load_network(File)\n",
        "\n",
        "    def load_network(self, File):\n",
        "        prob_table = defaultdict(lambda: defaultdict(odict))\n",
        "        with open(File, 'r') as file:\n",
        "            x = file.read()\n",
        "            m = self.node_pattern.findall(x)\n",
        "            for tup in m:\n",
        "                dom = tup[1].replace('\"', \" \")\n",
        "                self.outcomeSpace[tup[0]] = dom.split()\n",
        "\n",
        "            prob = self.prob_pattern.findall(x)\n",
        "\n",
        "            for tup in prob:\n",
        "                dom = tup[0].replace(\"|\", \" \")\n",
        "                dom = dom.split()\n",
        "\n",
        "                prob = tup[1].replace(\"(\", \" \")\n",
        "                prob = prob.replace(\")\", \" \")\n",
        "                prob = prob.split()\n",
        "\n",
        "                prob_table[dom[0]]['dom'] = tuple(dom)\n",
        "\n",
        "                l = [self.outcomeSpace[i] for i in dom]\n",
        "                x = list(product(*l))\n",
        "                for i in range(len(x)):\n",
        "                    prob_table[dom[0]]['table'][x[i]] = float(prob[i])\n",
        "\n",
        "                self.child_graph[dom[0]] = []\n",
        "                for i in range(1, len(dom)):\n",
        "                    self.child_graph[dom[i]].append(dom[0])\n",
        "\n",
        "        return prob_table\n",
        "\n",
        "    def insert_node(self, Node, Domain):\n",
        "        self.outcomeSpace[Node] = Domain\n",
        "        self.child_graph[Node] = []\n",
        "\n",
        "        table = odict()\n",
        "        p = 1  # /len(Domain)\n",
        "        for x in Domain:\n",
        "            table[(x, )] = p\n",
        "\n",
        "        self.prob_tables[Node] = {'dom': (Node,),\n",
        "                                  'table': table}\n",
        "\n",
        "        return\n",
        "\n",
        "    def remove_node(self, Node):\n",
        "        for v in self.child_graph[Node]:\n",
        "            self.disconnect_nodes(Node, v)\n",
        "\n",
        "        for v in self.prob_tables[Node]['dom'][1:]:\n",
        "            self.disconnect_nodes(v, Node)\n",
        "\n",
        "        del self.outcomeSpace[Node]\n",
        "        del self.prob_tables[Node]\n",
        "        del self.child_graph[Node]\n",
        "\n",
        "        return\n",
        "\n",
        "    def connect_nodes(self, Node1, Node2):\n",
        "        # Node1 is parent\n",
        "        # Node2 is child\n",
        "\n",
        "        self.child_graph[Node1].append(Node2)\n",
        "        # Join tables\n",
        "        prob_table = self.prob_tables[Node1]\n",
        "        temp_list = list(self.prob_tables[Node1]['dom'])\n",
        "        temp_list.remove(Node1)\n",
        "        for var in temp_list:\n",
        "            prob_table = marginalize(prob_table, var, self.outcomeSpace)\n",
        "        prob_table = normalize(prob_table)\n",
        "        self.prob_tables[Node2] = join(\n",
        "            self.prob_tables[Node2], prob_table, self.outcomeSpace)\n",
        "\n",
        "        return\n",
        "\n",
        "    def disconnect_nodes(self, Node1, Node2):\n",
        "        # Node1 is parent\n",
        "        # Node2 is child\n",
        "\n",
        "        self.child_graph[Node1].remove(Node2)\n",
        "        self.prob_tables[Node2] = marginalize(Node2, Node1, self.outcomeSpace)\n",
        "\n",
        "        return\n",
        "\n",
        "    def prob_node(self, Node, Prob):\n",
        "        # Prob should be dictionary with key as the tuple of value\n",
        "        # and value as the probability.\n",
        "        for k, v in Prob.items():\n",
        "            self.prob_tables[Node]['table'][k] = v\n",
        "\n",
        "        return\n",
        "\n",
        "    def save_network(self, filename):\n",
        "        with open(filename, 'w') as file:\n",
        "            file.write(\"net\\n{\\n}\\n\")\n",
        "            for node, domain in self.outcomeSpace.items():\n",
        "                file.write(\"node %s\\n\" % node)\n",
        "                dom = ''\n",
        "                for i in domain:\n",
        "                    dom += ' \\\"' + i + '\\\"'\n",
        "                file.write(\"{\\n  states = (%s);\\n}\\n\" % dom)\n",
        "\n",
        "            for node, v in self.prob_tables.items():\n",
        "                s = \" \".join(v['dom'][1:])\n",
        "                if s:\n",
        "                    node = node + ' | ' + s\n",
        "                file.write(\"potential ( %s )\\n{\\n\" % node)\n",
        "                prob = ' '.join([str(prob) for prob in v['table'].values()])\n",
        "                file.write(\"  data = ( %s ) ;\\n}\\n\" % prob)\n",
        "\n",
        "        return\n"
      ],
      "outputs": [],
      "execution_count": 202,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Representation\n",
        "The following code snippet demonstrate the representation of the Bayes net class.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def test_representation():\n",
        "    filename = \"./asia.net\"\n",
        "    b_net = Bayes_Network(filename)\n",
        "\n",
        "    output_file = \"./out_asia.net\"\n",
        "    b_net.save_network(output_file)\n",
        "\n",
        "test_representation()"
      ],
      "outputs": [],
      "execution_count": 203,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Contents of asia.net:\n",
        "\n",
        "    net \n",
        "    { \n",
        "    }\n",
        "    node asia \n",
        "    {\n",
        "      states = ( \"yes\" \"no\");\n",
        "    }\n",
        "    ...\n",
        "    node dysp \n",
        "    {\n",
        "      states = ( \"yes\" \"no\" );\n",
        "    }\n",
        "    \n",
        "    \n",
        "    potential ( asia ) \n",
        "    {\n",
        "      data = ( 0.01 0.99 );\n",
        "    }\n",
        "    ...\n",
        "    potential ( dysp | bronc either ) \n",
        "    {\n",
        "      data = (((0.9 0.1)(0.8 0.2))((0.7 0.3)(0.1 0.9))) ;\n",
        "    }\n",
        "\n",
        "#### Contents of output file out_asia.net:\n",
        "\n",
        "    net \n",
        "    { \n",
        "    }\n",
        "    node asia \n",
        "    {\n",
        "      states = ( \"yes\" \"no\");\n",
        "    }\n",
        "    ...\n",
        "    node dysp \n",
        "    {\n",
        "      states = ( \"yes\" \"no\" );\n",
        "    }\n",
        "    \n",
        " The probabilities are in the order of all True (TTT..) to all false (FFF..)\n",
        "    \n",
        "    potential ( asia ) \n",
        "    {\n",
        "      data = ( 0.01 0.99 );\n",
        "    }\n",
        "    ...\n",
        "    potential ( dysp | bronc either ) \n",
        "    {\n",
        "      data = ( 0.9 0.1 0.8 0.2 0.7 0.3 0.1 0.9 ) ;\n",
        "    }"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Functionalities of Bayes Net Class\n",
        "The following code snippet demonstrate the functionalities of the Bayes net class."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def test_functionalities():\n",
        "    filename = \"./test1.net\"\n",
        "    b_net = Bayes_Network(filename)\n",
        "\n",
        "    output_file = \"./out_test.net\"\n",
        "\n",
        "    Node1 = 'Xray'\n",
        "    b_net.insert_node(Node1, ['positive', 'negative'])\n",
        "    \n",
        "#     print('New Prob Table1:')\n",
        "#     pprint.pprint(b_net.prob_tables)\n",
        "    \n",
        "    b_net.connect_nodes('Cancer', 'Xray')\n",
        "\n",
        "#     print('New Prob Table2:')\n",
        "#     pprint.pprint(b_net.prob_tables)\n",
        "#     b_net.save_network(output_file)\n",
        "\n",
        "#     b_net.remove_node(Node1)\n",
        "    b_net.save_network(output_file)\n",
        "\n",
        "test_functionalities()"
      ],
      "outputs": [],
      "execution_count": 204,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning and Pre-Processing Techniques For Inference\n",
        "\n",
        "1. Network pruning techniques based on query structure. This technique is composed of edge and node pruning.\n",
        "2. Min-fill heuristic for variable elimination, using min-degree heuristics to break ties."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Pruning(Bayes_Network):\n",
        "    def __init__(self, filename):\n",
        "        super().__init__(filename)\n",
        "        self.induced_graph = defaultdict(list)\n",
        "        self.factors = self.factorisation()\n",
        "\n",
        "    def factorisation(self):\n",
        "        return [i['dom'] for i in self.prob_tables.values() if len(i['dom']) > 1]\n",
        "\n",
        "    def moralisation(self):\n",
        "        for node, v in self.prob_tables.items():\n",
        "            parents = sorted(v['dom'])\n",
        "\n",
        "            if len(parents) > 1:\n",
        "                for i in range(len(parents)):\n",
        "                    for j in range(i+1, len(parents)):\n",
        "                        self.induced_graph[parents[i]].append(parents[j])\n",
        "                        self.induced_graph[parents[j]].append(parents[i])\n",
        "\n",
        "        self.induced_graph = {k: sorted(v)\n",
        "                              for k, v in self.induced_graph.items()}\n",
        "\n",
        "        return\n",
        "\n",
        "    def prune_edge(self, Pruned_network, Node):\n",
        "        del Pruned_network[Node]\n",
        "        return\n",
        "\n",
        "    def prune_nodes(self, Pruned_network, Node, R):\n",
        "        for v in self.prob_tables[Node]['dom'][1:]:\n",
        "            Pruned_network[v].remove(Node)\n",
        "            if not Pruned_network[v] and v not in R:\n",
        "                self.prune_nodes(v, R)\n",
        "\n",
        "        return\n",
        "\n",
        "    def pruning(self, Query, **Evidence):\n",
        "        pruned_network = self.child_graph.copy()\n",
        "        retained_nodes = [Query] + list(Evidence.keys())\n",
        "\n",
        "        for node, children in self.child_graph.items():\n",
        "            if not children and node not in Query and node not in Evidence:\n",
        "                self.prune_nodes(pruned_network, node, retained_nodes)\n",
        "\n",
        "        for node in Evidence:\n",
        "            self.prune_edge(pruned_network, node)\n",
        "\n",
        "        return pruned_network\n",
        "\n",
        "    def node_degree(self):\n",
        "        node_degree = defaultdict(int)\n",
        "        for n, nodes in self.induced_graph.items():\n",
        "            node_degree[n] += len(nodes)\n",
        "            for node in nodes:\n",
        "                node_degree[node] += 1\n",
        "\n",
        "        return node_degree\n",
        "\n",
        "    def min_fill(self):\n",
        "        node_min_fill = defaultdict(list)\n",
        "        node_degree = self.node_degree()\n",
        "\n",
        "        for n, nodes in self.induced_graph.items():\n",
        "            node_min_fill[n].append(sum([1 for i in range(len(nodes))\n",
        "                                         for j in range(i+1, len(nodes))\n",
        "                                         if not nodes[j] in self.induced_graph[nodes[i]]]))\n",
        "            node_min_fill[n].append(node_degree[n])\n",
        "\n",
        "        return sorted(node_min_fill, key=node_min_fill.get)\n"
      ],
      "outputs": [],
      "execution_count": 205,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Pruning\n",
        "The following code snippet demonstrate the Pruning technique to speed up the inference. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def test_pruning():\n",
        "    filename = \"./asia.net\"\n",
        "    b_net = Pruning(filename)\n",
        "    b_net.moralisation()\n",
        "    b_net.pruning(\"tub\", bronc=\"yes\", either=\"yes\")\n",
        "    print(b_net.min_fill())\n",
        "\n",
        "test_pruning()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['asia', 'xray', 'dysp', 'smoke', 'tub', 'lung', 'bronc', 'either']\n"
          ]
        }
      ],
      "execution_count": 206,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exact Inference\n",
        "Implemented the Jointree Algorithm using the Shenoy-Shaffer architecture. The steps followed:\n",
        "- Convert an elimination order into a join tree\n",
        "- Join Tree Transformations: Add/Merge/Remove Clusters\n",
        "- Answer query based on the Join Tree.\n",
        "- Set evidence"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Exact_Inference(Bayes_Network):\n",
        "    def __init__(self, File, Elimination_order):\n",
        "        super().__init__(File)\n",
        "        self.factors = self.factorisation()\n",
        "        self.clusters = defaultdict(lambda: defaultdict())\n",
        "        self.join_tree = self.make_jointree(Elimination_order)\n",
        "\n",
        "    def factorisation(self):\n",
        "        return {i['dom']: {'dom': i['dom'],\n",
        "                           'table': i['table']} for i in self.prob_tables.values() if len(i['dom']) > 1}\n",
        "\n",
        "    def make_jointree(self, Elimination_order):\n",
        "        jointree = defaultdict(lambda: defaultdict())\n",
        "\n",
        "        clusters = self.add_clusters(Elimination_order)\n",
        "        clusters = self.merge_clusters(clusters)\n",
        "\n",
        "        union_set = set()\n",
        "        for cluster in reversed(clusters):\n",
        "            if not union_set:\n",
        "                jointree[tuple(cluster)] = defaultdict()\n",
        "\n",
        "            else:\n",
        "                intersection_set = set(cluster) & union_set\n",
        "\n",
        "                for k in list(jointree.keys()):\n",
        "                    if intersection_set.issubset(k):\n",
        "                        jointree[k][tuple(cluster)] = defaultdict()\n",
        "                        jointree[tuple(cluster)][k] = defaultdict()\n",
        "\n",
        "            union_set |= set(c for c in cluster)\n",
        "\n",
        "        return jointree\n",
        "\n",
        "    def set_evidence(self, Evidence):\n",
        "        newOutcomeSpace = self.outcomeSpace.copy()\n",
        "        for var, e in Evidence.items():\n",
        "            newOutcomeSpace[var] = (e,)\n",
        "        return newOutcomeSpace\n",
        "\n",
        "    def merge_clusters(self, clusters):\n",
        "        seen_list = []\n",
        "        for cluster in clusters:\n",
        "            seen_list.append(set(cluster))\n",
        "            for item in seen_list[:-1]:\n",
        "                if cluster < item:\n",
        "                    seen_list.remove(item)\n",
        "                    seen_list.remove(set(cluster))\n",
        "                    seen_list.append(item)\n",
        "                    break\n",
        "                if item <= cluster:\n",
        "                    seen_list.remove(item)\n",
        "                    break\n",
        "\n",
        "        return seen_list\n",
        "\n",
        "    def add_clusters(self, Elimination_order):\n",
        "        clusters = []\n",
        "        eliminated_nodes = []\n",
        "        temp_list = list(self.factors.keys())\n",
        "        for node in Elimination_order:\n",
        "            lis = set(factor for factors in temp_list\n",
        "                      for factor in factors\n",
        "                      if node in factors)\n",
        "\n",
        "            cluster = lis - set(eliminated_nodes)\n",
        "            clusters.append(cluster)\n",
        "            \n",
        "            for factors, v in self.factors.items():\n",
        "                if set(factors) <= set(cluster):\n",
        "                    if not self.clusters[tuple(cluster)]:\n",
        "                        self.clusters[tuple(cluster)] = v\n",
        "                    else:\n",
        "                        self.clusters[tuple(cluster)] = join(self.clusters[tuple(cluster)], v, self.outcomeSpace)\n",
        "\n",
        "            flag = 0\n",
        "            for i in range(len(self.child_graph[node])):\n",
        "                for j in range(i+1, len(self.child_graph[node])):\n",
        "                    for factors in temp_list:\n",
        "                        if self.child_graph[node][i] in factors and self.child_graph[node][j] in factors:\n",
        "                            flag += 1\n",
        "                    if not flag:\n",
        "                        temp_list.append(\n",
        "                            (self.child_graph[node][i], self.child_graph[node][j]))\n",
        "\n",
        "            eliminated_nodes.append(node)\n",
        "\n",
        "        return clusters\n",
        "\n",
        "    def remove_cluster(self):\n",
        "        return\n",
        "\n",
        "    def in_list(self, A, B):\n",
        "        if not A or not B:\n",
        "            return True\n",
        "        for a in A:\n",
        "            if a in B:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def compute_message(self, cluster, outcomeSpace):\n",
        "        message = self.clusters[cluster]\n",
        "        for k, v in self.join_tree[cluster].items():\n",
        "            if v:\n",
        "                message = join(message, v, outcomeSpace)\n",
        "\n",
        "        return message\n",
        "\n",
        "    def send_message(self, cluster1, cluster2, outcomeSpace):\n",
        "\n",
        "        message = self.clusters[cluster1]\n",
        "        domain = set(cluster1) - set(cluster2)\n",
        "        for k, v in self.join_tree[cluster1].items():\n",
        "            if v:\n",
        "                message = join(message, v, outcomeSpace)\n",
        "                for var in list(set(message['dom']) - domain):\n",
        "                    message = marginalize(message, var, outcomeSpace)\n",
        "\n",
        "        self.join_tree[cluster2][cluster1] = message\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate_pass_order(self, current, queried, message_order, seen_list):\n",
        "        for cluster in self.join_tree[current]:\n",
        "            if cluster not in seen_list and self.in_list(queried, cluster):\n",
        "                seen_list.append(cluster)\n",
        "                message_order.append(cluster)\n",
        "                self.generate_pass_order(\n",
        "                    cluster, queried, message_order, seen_list)\n",
        "\n",
        "        return\n",
        "\n",
        "    def ans_query(self, Query, **Evidence):\n",
        "\n",
        "        leaf_clusters = [k for k, v in self.join_tree.items()\n",
        "                         if len(v) == 1]\n",
        "\n",
        "        outcomeSpace = self.set_evidence(Evidence)\n",
        "\n",
        "        evidence = list(Evidence.keys())\n",
        "        flag = 0\n",
        "        root = None\n",
        "        for cluster in self.join_tree.keys():\n",
        "            if flag:\n",
        "                break\n",
        "            if Query in cluster and self.in_list(evidence, cluster):\n",
        "                flag = 1\n",
        "                root = cluster\n",
        "            elif Query in cluster:\n",
        "                root = cluster\n",
        "\n",
        "        message_order = []\n",
        "        if flag:\n",
        "            queried = [Query] + evidence\n",
        "            self.generate_pass_order(root, queried, message_order, [])\n",
        "        else:\n",
        "            queried = list(self.outcomeSpace.keys())\n",
        "            self.generate_pass_order(root, queried, message_order, [])\n",
        "\n",
        "        length = len(message_order)\n",
        "        for i in range(length-2):\n",
        "            self.send_message(\n",
        "                message_order[length-i-1], message_order[length-i-2], outcomeSpace)\n",
        "\n",
        "        output = self.clusters[root]\n",
        "        for var in output['dom']:\n",
        "            if var != Query:\n",
        "                output = marginalize(output, var, outcomeSpace)\n",
        "\n",
        "        return normalize(output)\n"
      ],
      "outputs": [],
      "execution_count": 207,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Exact Inference\n",
        "The following code snippet demonstrates the method of exact inference of the given query."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def test_exact_inference():\n",
        "    filename = \"./munin.net\"\n",
        "    b_net = Pruning(filename)\n",
        "    b_net.moralisation()\n",
        "    x = Exact_Inference(filename, b_net.min_fill())\n",
        "    printFactor(x.ans_query('R_LNLW_MED_SEV', DIFFN_SEV=\"MOD\"))\n",
        "    return\n",
        "\n",
        "\n",
        "test_exact_inference()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| R_LNLW_MED_SEV   |     Pr |\n",
            "|------------------+--------|\n",
            "| NO               | 0.2    |\n",
            "| MILD             | 0.2095 |\n",
            "| MOD              | 0.1905 |\n",
            "| SEV              | 0.2    |\n",
            "| TOTAL            | 0.2    |\n"
          ]
        }
      ],
      "execution_count": 187,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# def test_exact_inference():\n",
        "#     filename = \"./asia.net\"\n",
        "#     b_net = Pruning(filename)\n",
        "#     b_net.moralisation()\n",
        "#     x = Exact_Inference(filename, b_net.min_fill())\n",
        "#     print(x.ans_query('lung', asia=\"yes\", tub=\"no\"))\n",
        "#     return\n",
        "\n",
        "\n",
        "# def test_exact_inference():\n",
        "#     filename = \"./asia.net\"\n",
        "#     b_net = Pruning(filename)\n",
        "#     b_net.moralisation()\n",
        "#     x = Exact_Inference(filename, b_net.min_fill())\n",
        "#     print(x.ans_query('lung', asia=\"yes\", tub=\"no\"))\n",
        "#     # print(x.ans_query('L_OTHER_ISCH_DIFSLOW', L_SUR_ALLCV_CA='M_S52'))#, asia=\"yes\", tub=\"no\"))\n",
        "#     return\n",
        "\n",
        "\n",
        "# test_exact_inference()"
      ],
      "outputs": [],
      "execution_count": 188,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approximate inference\n",
        "###  Iterative Joingraph Propagation (IJGP)\n",
        "* Create Bethe cluster graph and use it as join graph\n",
        "* Answer query based on join graph clusters\n",
        "* Set evidence"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Approx_Inference(Bayes_Network):\n",
        "    def __init__(self, File):\n",
        "        super().__init__(File)\n",
        "        self.factors = self.factorisation()\n",
        "        self.clusters = defaultdict(lambda: defaultdict())\n",
        "        self.bethe_graph = self.create_Bethe_graph()\n",
        "\n",
        "    def factorisation(self):\n",
        "        return {i['dom']: {'dom': i['dom'],\n",
        "                           'table': i['table']} for i in self.prob_tables.values() if len(i['dom']) > 1}\n",
        "\n",
        "    def create_Bethe_graph(self):\n",
        "        cluster_graph = defaultdict(lambda: defaultdict())\n",
        "        clusters = []\n",
        "        singleton_clusters = []\n",
        "\n",
        "        for node in self.outcomeSpace.keys():\n",
        "            singleton_clusters.append((node,))\n",
        "\n",
        "        for factors in self.factors:\n",
        "            for c in singleton_clusters:\n",
        "                if c[0] in factors:\n",
        "                    cluster_graph[factors][c] = defaultdict()\n",
        "                    cluster_graph[c][factors] = defaultdict()\n",
        "            clusters.append(factors)\n",
        "\n",
        "        for k, v in self.factors.items():\n",
        "            self.clusters[k] = v\n",
        "\n",
        "        for cluster in singleton_clusters:\n",
        "            self.clusters[cluster] = self.prob_tables[cluster[0]].copy()\n",
        "            for var in self.prob_tables[cluster[0]]['dom']:\n",
        "                if var not in cluster:\n",
        "                    self.clusters[cluster] = marginalize(\n",
        "                        self.clusters[cluster], var, self.outcomeSpace)\n",
        "\n",
        "        return cluster_graph\n",
        "\n",
        "    def add_clusters(self, clusters):\n",
        "\n",
        "        return\n",
        "\n",
        "    def set_evidence(self, Evidence):\n",
        "        newOutcomeSpace = self.outcomeSpace.copy()\n",
        "        for var, e in Evidence.items():\n",
        "            newOutcomeSpace[var] = (e,)\n",
        "        return newOutcomeSpace\n",
        "\n",
        "    def in_list(self, A, B):\n",
        "        if not A or not B:\n",
        "            return True\n",
        "        for a in A:\n",
        "            if a in B:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def cluster_message(self, root, outcomeSpace, visited_clusters):\n",
        "        visited_clusters.append(root)\n",
        "        prob_table = self.clusters[root]\n",
        "        for cluster in self.bethe_graph[root].keys():\n",
        "            if cluster not in visited_clusters:\n",
        "                if self.clusters[cluster]['table']:\n",
        "                    prob_table = join(\n",
        "                        prob_table, self.clusters[cluster], outcomeSpace)\n",
        "                else:\n",
        "                    self.clusters[cluster] = self.cluster_message(\n",
        "                        cluster, outcomeSpace, visited_clusters)\n",
        "                    prob_table = join(\n",
        "                        prob_table, self.clusters[cluster], outcomeSpace)\n",
        "\n",
        "        return prob_table\n",
        "\n",
        "    def send_message(self, cluster1, cluster2, outcomeSpace):\n",
        "        # From cluster 1 to cluster 2\n",
        "\n",
        "        message = self.clusters[cluster1]\n",
        "        domain = set(cluster1) - set(cluster2)\n",
        "        for k, v in self.bethe_graph[cluster1].items():\n",
        "            if v:\n",
        "                message = join(message, v, outcomeSpace)\n",
        "                for var in list(set(message['dom']) - domain):\n",
        "                    message = marginalize(message, var, outcomeSpace)\n",
        "\n",
        "        self.bethe_graph[cluster2][cluster1] = message\n",
        "\n",
        "        return\n",
        "\n",
        "    def generate_pass_order(self, current, queried, message_order, seen_list):\n",
        "        for cluster in self.bethe_graph[current]:\n",
        "            if cluster not in seen_list and self.in_list(queried, cluster):\n",
        "                seen_list.append(cluster)\n",
        "                message_order.append(cluster)\n",
        "                self.generate_pass_order(\n",
        "                    cluster, queried, message_order, seen_list)\n",
        "\n",
        "        return\n",
        "\n",
        "    def check_convergence(self, graph, message, delta, outcomeSpace):\n",
        "        if not graph:\n",
        "            return\n",
        "\n",
        "        temp_val = graph.copy()\n",
        "        if graph['dom'] != message['dom']:\n",
        "            for var in list(set(graph['dom']) - set(message['dom'])):\n",
        "                temp_val = marginalize(temp_val, var, outcomeSpace)\n",
        "\n",
        "        for k, v in message['table'].items():\n",
        "            if abs(temp_val['table'][k] - v) > delta:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def update_message(self, graph, outcomeSpace, delta):\n",
        "        for k, v in graph.items():\n",
        "            for cluster, messages in v.items():\n",
        "                message = self.clusters[k]\n",
        "\n",
        "                for c, m in v.items():\n",
        "                    if c != cluster and m:\n",
        "                        message = join(message, m, outcomeSpace)\n",
        "                        for var in message['dom']:\n",
        "                            if var not in list(set(k) & set(cluster)):\n",
        "                                message = marginalize(\n",
        "                                    message, var, outcomeSpace)\n",
        "                message = normalize(message)\n",
        "                if self.check_convergence(graph[k][cluster], message, delta, outcomeSpace):\n",
        "                    graph[k].pop(cluster, None)\n",
        "                graph[k][cluster] = message\n",
        "                self.bethe_graph[k][cluster] = message\n",
        "\n",
        "        return\n",
        "\n",
        "    def ans_query(self, delta, num_iters, Query, **Evidence):\n",
        "\n",
        "        outcomeSpace = self.set_evidence(Evidence)\n",
        "        temp_graph = self.bethe_graph.copy()\n",
        "\n",
        "        iteration = 0\n",
        "        while temp_graph and iteration < num_iters:\n",
        "            self.update_message(temp_graph, outcomeSpace, delta)\n",
        "            iteration += 1\n",
        "\n",
        "        for cluster in self.bethe_graph.keys():\n",
        "            if Query in cluster:\n",
        "                root = cluster\n",
        "                break\n",
        "\n",
        "        evidence = list(Evidence.keys())\n",
        "        flag = 0\n",
        "        root = None\n",
        "        for cluster in temp_graph.keys():\n",
        "            if flag:\n",
        "                break\n",
        "            if Query in cluster and self.in_list(evidence, cluster):\n",
        "                flag = 1\n",
        "                root = cluster\n",
        "            elif Query in cluster:\n",
        "                root = cluster\n",
        "\n",
        "        message_order = []\n",
        "        if flag:\n",
        "            queried = [Query] + evidence\n",
        "            self.generate_pass_order(root, queried, message_order, [])\n",
        "        else:\n",
        "            queried = list(self.outcomeSpace.keys())\n",
        "            self.generate_pass_order(root, queried, message_order, [])\n",
        "\n",
        "        length = len(message_order)\n",
        "\n",
        "        for i in range(length-2):\n",
        "            self.send_message(\n",
        "                message_order[length-i-1], message_order[length-i-2], outcomeSpace)\n",
        "\n",
        "        output = self.clusters[root]\n",
        "        for var in output['dom']:\n",
        "            if var != Query:\n",
        "                output = marginalize(output, var, outcomeSpace)\n",
        "\n",
        "        return normalize(output)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 208,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Approximate Inference using Iterative Join Graph Propagation Technique\n",
        "The following code snippet demonstrates the method of approximate inference of the given query, following the Iterative Join Graph Propagation technique."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def test_approx_inference():\n",
        "    filename = \"./asia.net\"\n",
        "    b_net = Pruning(filename)\n",
        "    b_net.moralisation()\n",
        "    x = Approx_Inference(filename)\n",
        "    printFactor(x.ans_query(0.01, 100, 'tub', asia=\"yes\"))\n",
        "    return\n",
        "\n",
        "\n",
        "# def test_approx_inference():\n",
        "#     filename = \"./asia.net\"\n",
        "#     b_net = Pruning(filename)\n",
        "#     b_net.moralisation()\n",
        "#     x = Approx_Inference(filename)\n",
        "#     x.ans_query('tub', asia=\"yes\")\n",
        "#     return\n",
        "\n",
        "test_approx_inference()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| tub   |       Pr |\n",
            "|-------+----------|\n",
            "| yes   | 0.833333 |\n",
            "| no    | 0.166667 |\n"
          ]
        }
      ],
      "execution_count": 209,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark\n",
        "\n",
        "\n",
        "The inference techniques will be compared by analysing it over Bayes net of various sizes. We have chosen the following networks to draw a contrast:\n",
        "\n",
        "1. Small Network       :  CANCER\n",
        "2. Medium Network      :  CHILD\n",
        "3. Large Network       :  HAILFINDER\n",
        "4. Very Large Network  :  PATHFINDER\n",
        "5. Massive Network     :  MUNIN (Full Network)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(graph, exact_result, approx_result):\n",
        "    for k,v in exact_result['table'].items():\n",
        "        graph.append((v, approx_result['table'][k]))\n",
        "    return\n",
        "\n",
        "graph = []"
      ],
      "outputs": [],
      "execution_count": 214,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Small Network (< 20 Nodes)\n",
        "* Bayes Net: CANCER\n",
        "* Number of nodes:\t \t5\n",
        "* Number of arcs: \t\t4\n",
        "* Number of parameters\t: \t10\n",
        "* Average Markov blanket size: \t2.00\n",
        "* Average degree: \t\t1.60\n",
        "* Maximum in-degree: \t\t2\n",
        "\n",
        "Query:\n",
        "> P(Xray | Cancer = 'True')"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Query: CANCER\n",
        "filename = \"./cancer.net\"\n",
        "\n",
        "start_time = time.time()\n",
        "print('Exact Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "exact_inf = Exact_Inference(filename, b_net.min_fill())\n",
        "result1 = exact_inf.ans_query('Xray', Cancer=\"True\")\n",
        "# print(\"P(Xray | Cancer = 'True') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result1)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "print('Approximate Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "approx_inf = Approx_Inference(filename)\n",
        "result2 = approx_inf.ans_query(0.01, 100, 'Xray', Cancer=\"True\")\n",
        "# print(\"P(Xray | Cancer = 'True') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result2)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "plot_graph(graph, result1, result2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Inference:\n",
            "| Xray     |       Pr |\n",
            "|----------+----------|\n",
            "| positive | 0.818182 |\n",
            "| negative | 0.181818 |\n",
            "Time: 0.0029702186584472656\n",
            "\n",
            "Approximate Inference:\n",
            "| Xray     |       Pr |\n",
            "|----------+----------|\n",
            "| positive | 0.818182 |\n",
            "| negative | 0.181818 |\n",
            "Time: 0.05326724052429199\n"
          ]
        }
      ],
      "execution_count": 215,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Medium Network (20 - 50 Nodes)\n",
        "* Bayes Net: CHILD\n",
        "* Number of nodes:\t \t20\n",
        "* Number of arcs: \t\t25\n",
        "* Number of parameters\t: \t230\n",
        "* Average Markov blanket size: \t3.00\n",
        "* Average degree: \t\t1.25\n",
        "* Maximum in-degree: \t\t2\n",
        "\n",
        "Query:\n",
        "> P(LungFlow | Disease = 'Lung')"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Query: CHILD\n",
        "filename = \"./child.net\"\n",
        "\n",
        "start_time = time.time()\n",
        "print('Exact Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "exact_inf = Exact_Inference(filename, b_net.min_fill())\n",
        "result1 = exact_inf.ans_query('LungFlow', Disease=\"Lung\")\n",
        "# print(\"P(LungFlow | Disease = 'Lung') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result1)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "print('Approximate Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "approx_inf = Approx_Inference(filename)\n",
        "result2 = approx_inf.ans_query(0.01, 100, 'LungFlow', Disease=\"Lung\")\n",
        "# print(\"P(LungFlow | Disease = 'Lung') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result2)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "plot_graph(graph, result1, result2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Inference:\n",
            "| LungFlow   |        Pr |\n",
            "|------------+-----------|\n",
            "| Normal     | 0.883686  |\n",
            "| Low        | 0.0455467 |\n",
            "| High       | 0.0707672 |\n",
            "Time: 0.009591102600097656\n",
            "\n",
            "Approximate Inference:\n",
            "| LungFlow   |   Pr |\n",
            "|------------+------|\n",
            "| Normal     | 0.75 |\n",
            "| Low        | 0.05 |\n",
            "| High       | 0.2  |\n",
            "Time: 0.6121549606323242\n"
          ]
        }
      ],
      "execution_count": 217,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Large Network (50 - 100 Nodes)\n",
        "* Bayes Net: HAILFINDER\n",
        "* Number of nodes:\t \t56\n",
        "* Number of arcs: \t\t66\n",
        "* Number of parameters\t: \t2656\n",
        "* Average Markov blanket size: \t3.54\n",
        "* Average degree: \t\t2.36\n",
        "* Maximum in-degree: \t\t4\n",
        "\n",
        "Query:\n",
        "> P(TempDis | Scenario = 'C')"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Query: HAILFINDER\n",
        "filename = \"./hailfinder.net\"\n",
        "\n",
        "start_time = time.time()\n",
        "print('Exact Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "exact_inf = Exact_Inference(filename, b_net.min_fill())\n",
        "result1 = exact_inf.ans_query('TempDis', Scenario=\"C\")\n",
        "# print(\"P(TempDis | Scenario = 'C') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result1)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "print('Approximate Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "approx_inf = Approx_Inference(filename)\n",
        "result2 = approx_inf.ans_query(0.01, 100, 'TempDis', Scenario=\"C\")\n",
        "# print(\"P(TempDis | Scenario = 'C') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result2)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "plot_graph(graph, result1, result2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Inference:\n",
            "| TempDis     |        Pr |\n",
            "|-------------+-----------|\n",
            "| QStationary | 0.344828  |\n",
            "| Moving      | 0.517241  |\n",
            "| None        | 0.103448  |\n",
            "| Other       | 0.0344828 |\n",
            "Time: 0.07608699798583984\n",
            "\n",
            "Approximate Inference:\n",
            "| TempDis     |        Pr |\n",
            "|-------------+-----------|\n",
            "| QStationary | 0.344828  |\n",
            "| Moving      | 0.517241  |\n",
            "| None        | 0.103448  |\n",
            "| Other       | 0.0344828 |\n",
            "Time: 7.7873358726501465\n"
          ]
        }
      ],
      "execution_count": 218,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Very Large Network (100 - 1000 Nodes)\n",
        "* Bayes Net: PATHFINDER\n",
        "* Number of nodes:\t \t109\n",
        "* Number of arcs: \t\t195\n",
        "* Number of parameters\t: \t72079\n",
        "* Average Markov blanket size: \t3.81\n",
        "* Average degree: \t\t3.57\n",
        "* Maximum in-degree: \t\t5\n",
        "\n",
        "Query:\n",
        "> P(F108 | Fault = 'Infectious_mono')"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Query: PATHFINDER\n",
        "filename = \"./pathfinder.net\"\n",
        "\n",
        "start_time = time.time()\n",
        "print('Exact Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "exact_inf = Exact_Inference(filename, b_net.min_fill())\n",
        "result1 = exact_inf.ans_query('F108', Fault=\"Infectious_mono\")\n",
        "# print(\"P(F108 | Fault = 'Infectious_mono') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result1)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "print('Approximate Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "approx_inf = Approx_Inference(filename)\n",
        "result2 = approx_inf.ans_query(0.01, 100, 'F108', Fault=\"Infectious_mono\")\n",
        "# print(\"P(F108 | Fault = 'Infectious_mono') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result2)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "plot_graph(graph, result1, result2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Inference:\n",
            "| F108    |   Pr |\n",
            "|---------+------|\n",
            "| Absent  |  0.5 |\n",
            "| Present |  0.5 |\n",
            "Time: 0.8416707515716553\n",
            "\n",
            "Approximate Inference:\n",
            "| F108    |   Pr |\n",
            "|---------+------|\n",
            "| Absent  |  0.5 |\n",
            "| Present |  0.5 |\n",
            "Time: 22.22428321838379\n"
          ]
        }
      ],
      "execution_count": 219,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Massive Network (> 1000 Nodes)\n",
        "* Bayes Net: MUNIN (Full Network)\n",
        "* Number of nodes:\t \t1041\n",
        "* Number of arcs: \t\t1397\n",
        "* Number of parameters\t: \t80592\n",
        "* Average Markov blanket size: \t3.54\n",
        "* Average degree: \t\t2.68\n",
        "* Maximum in-degree: \t\t3\n",
        "\n",
        "Query:\n",
        "> P(L_SUR_CV_CA | L_SUR_ALLCV_CA = 'M_S44')"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Query: MUNIN (Full Network)\n",
        "filename = \"./munin.net\"\n",
        "\n",
        "start_time = time.time()\n",
        "print('Exact Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "exact_inf = Exact_Inference(filename, b_net.min_fill())\n",
        "result1 = exact_inf.ans_query('L_SUR_CV_CA', L_SUR_ALLCV_CA=\"M_S44\")\n",
        "# print(\"P(L_SUR_CV_CA | L_SUR_ALLCV_CA = 'M_S44') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result1)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "print()\n",
        "\n",
        "start_time = time.time()\n",
        "print('Approximate Inference:')\n",
        "b_net = Pruning(filename)\n",
        "b_net.moralisation()\n",
        "approx_inf = Approx_Inference(filename)\n",
        "result2 = approx_inf.ans_query(0.01, 100, 'L_SUR_CV_CA', L_SUR_ALLCV_CA=\"M_S44\")\n",
        "# print(\"P(L_SUR_CV_CA | L_SUR_ALLCV_CA = 'M_S44') =\", result)\n",
        "# pprint.pprint(result)\n",
        "printFactor(result2)\n",
        "print('Time:', time.time() - start_time)\n",
        "\n",
        "plot_graph(graph, result1, result2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Inference:\n",
            "| L_SUR_CV_CA   |          Pr |\n",
            "|---------------+-------------|\n",
            "| M_S00         | 0           |\n",
            "| M_S04         | 0.0431844   |\n",
            "| M_S08         | 0           |\n",
            "| M_S12         | 0.36417     |\n",
            "| M_S16         | 0           |\n",
            "| M_S20         | 0.00342768  |\n",
            "| M_S24         | 0.000685535 |\n",
            "| M_S28         | 0           |\n",
            "| M_S32         | 0.584533    |\n",
            "| M_S36         | 0           |\n",
            "| M_S40         | 0.00399936  |\n",
            "| M_S44         | 0           |\n",
            "| M_S48         | 0           |\n",
            "| M_S52         | 0           |\n",
            "| M_S56         | 0           |\n",
            "| M_S60         | 0           |\n",
            "| M_S_64        | 0           |\n",
            "Time: 2.557680130004883\n",
            "\n",
            "Approximate Inference:\n",
            "| L_SUR_CV_CA   |          Pr |\n",
            "|---------------+-------------|\n",
            "| M_S00         | 0           |\n",
            "| M_S04         | 0.0431844   |\n",
            "| M_S08         | 0           |\n",
            "| M_S12         | 0.36417     |\n",
            "| M_S16         | 0           |\n",
            "| M_S20         | 0.00342768  |\n",
            "| M_S24         | 0.000685535 |\n",
            "| M_S28         | 0           |\n",
            "| M_S32         | 0.584533    |\n",
            "| M_S36         | 0           |\n",
            "| M_S40         | 0.00399936  |\n",
            "| M_S44         | 0           |\n",
            "| M_S48         | 0           |\n",
            "| M_S52         | 0           |\n",
            "| M_S56         | 0           |\n",
            "| M_S60         | 0           |\n",
            "| M_S_64        | 0           |\n",
            "Time: 188.49355101585388\n"
          ]
        }
      ],
      "execution_count": 221,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"Exact Inference\")\n",
        "plt.ylabel(\"Approximate Inference\")\n",
        "plt.scatter(*zip(*graph))\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": [
              "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\n"
            ],
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 222,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CONCLUSION\n",
        "\n",
        "The above results show that the output from exact inference and approximate inference methods are very close to each other.\n",
        "\n",
        "The other interesting thing to note is that exact inference runs considerably faster than approximate inference. Part of this reason can be due to the factors being pre-calculated for the exact inference method and iterations being done to find the same in approximate inference method. But given these factors, the two methods have the same complexity according to our implementation."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "inputHidden": false,
        "outputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}